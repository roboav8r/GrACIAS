{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b348e5b-cb0e-4ed7-9582-70ebd0fe4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c5a16d-1b6f-4f1f-b13a-2f2e63762a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = [\"unknown\",\"pedestrian\",\"teammate\"]\n",
    "obs = [\"no_vest\",\"vest\"]\n",
    "id = 113\n",
    "n_obs = 0\n",
    "symbol = gtsam.Symbol('r',id)\n",
    "role = gtsam.DiscreteDistribution((symbol.key(),len(roles)),\".7/0/0\")\n",
    "\n",
    "values = gtsam.Values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369b597c-291b-420d-b0e0-51a61062917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size: 3\n",
      "factor 0:  P( r113 ):\n",
      " Choice(r113) \n",
      " 0 Leaf    1\n",
      " 1 Leaf    0\n",
      " 2 Leaf    0\n",
      "\n",
      "factor 1:  P( r113 | v1 ):\n",
      " Choice(v1) \n",
      " 0 Choice(r113) \n",
      " 0 0 Leaf 0.33333333\n",
      " 0 1 Leaf 0.66666667\n",
      " 0 2 Leaf    0\n",
      " 1 Choice(r113) \n",
      " 1 0 Leaf 0.33333333\n",
      " 1 1 Leaf    0\n",
      " 1 2 Leaf 0.66666667\n",
      "\n",
      "factor 2:  P( r113 | v2 ):\n",
      " Choice(v2) \n",
      " 0 Choice(r113) \n",
      " 0 0 Leaf 0.33333333\n",
      " 0 1 Leaf 0.66666667\n",
      " 0 2 Leaf    0\n",
      " 1 Choice(r113) \n",
      " 1 0 Leaf 0.33333333\n",
      " 1 1 Leaf    0\n",
      " 1 2 Leaf 0.66666667\n",
      "\n",
      "\n",
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_repr_html_', '_repr_markdown_', 'add', 'at', 'dot', 'eliminateMultifrontal', 'eliminatePartialMultifrontal', 'eliminatePartialSequential', 'eliminateSequential', 'empty', 'equals', 'keys', 'maxProduct', 'optimize', 'print', 'product', 'push_back', 'saveGraph', 'size', 'sumProduct']\n",
      "DiscreteBayesNet\n",
      " \n",
      "size: 3\n",
      "conditional 0:  P( v1 | r113 ):\n",
      " Choice(v1) \n",
      " 0 Choice(r113) \n",
      " 0 0 Leaf 0.45\n",
      " 0 1 Leaf    1\n",
      " 0 2 Leaf    0\n",
      " 1 Choice(r113) \n",
      " 1 0 Leaf 0.55\n",
      " 1 1 Leaf    0\n",
      " 1 2 Leaf    1\n",
      "\n",
      "conditional 1:  P( r113 | v2 ):\n",
      " Choice(v2) \n",
      " 0 Choice(r113) \n",
      " 0 0 Leaf    1\n",
      " 0 1 Leaf    0\n",
      " 0 2 Leaf    0\n",
      " 1 Choice(r113) \n",
      " 1 0 Leaf    1\n",
      " 1 1 Leaf    0\n",
      " 1 2 Leaf    0\n",
      "\n",
      "conditional 2:  P( v2 ):\n",
      " Choice(v2) \n",
      " 0 Leaf  0.4\n",
      " 1 Leaf  0.6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "role_dbn = gtsam.DiscreteBayesNet()\n",
    "\n",
    "role_dbn.add((symbol.key(),len(roles)),\".7/0/0\") # Prior estimate\n",
    "\n",
    "# Simulate adding a measurement\n",
    "obs_sym1 = gtsam.Symbol('v',1)\n",
    "role_dbn.add( (symbol.key(),len(roles)), [(obs_sym1.key(),len(obs))], \".5/1./0. .5/0./1.\")\n",
    "\n",
    "# simulate adding a second measurement\n",
    "obs_sym2 = gtsam.Symbol('v',2)\n",
    "role_dbn.add( (symbol.key(),len(roles)), [(obs_sym2.key(),len(obs))], \".5/1./0. .5/0./1.\")\n",
    "\n",
    "# Convert to graph, solve, and output values\n",
    "role_graph = gtsam.DiscreteFactorGraph(role_dbn)\n",
    "role_graph.optimize()\n",
    "\n",
    "print(role_graph)\n",
    "\n",
    "\n",
    "### NOW ADD EVIDENCE\n",
    "role_graph.add((obs_sym1.key(),len(obs)), \".45 .55\")\n",
    "role_graph.add((obs_sym2.key(),len(obs)), \".4 .6\")\n",
    "role_graph.optimize()\n",
    "\n",
    "print(dir(role_graph))\n",
    "\n",
    "print(role_graph.sumProduct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b69b6-acd1-4237-ba35-77dc3eaa3ad3",
   "metadata": {},
   "source": [
    "# Audio dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b526bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchaudio.io import StreamReader\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edbc920e-b552-4f1a-bdde-d25cf365bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(q, format, src, segment_length, sample_rate):\n",
    "    \n",
    "    print(\"Building StreamReader...\")\n",
    "    streamer = torchaudio.io.StreamReader(src=src, format=format, option=option)\n",
    "    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate, format=\"s16p\", num_channels=6)\n",
    "\n",
    "    print(streamer.get_src_stream_info(0))\n",
    "    print(\"Streaming...\")\n",
    "    print()\n",
    "    for (chunk_a) in streamer.stream(timeout=-1, backoff=1.0):\n",
    "        q.put([chunk_a])\n",
    "\n",
    "\n",
    "class ContextCacher:\n",
    "    \"\"\"Cache the end of input data and prepend the next input data with it.\n",
    "\n",
    "    Args:\n",
    "        segment_length (int): The size of main segment.\n",
    "            If the incoming segment is shorter, then the segment is padded.\n",
    "        context_length (int): The size of the context, cached and appended.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment_length: int, context_length: int):\n",
    "        self.segment_length = segment_length\n",
    "        self.context_length = context_length\n",
    "        self.context = torch.zeros([context_length])\n",
    "\n",
    "    def __call__(self, chunk: torch.Tensor):\n",
    "        if chunk.size(0) < self.segment_length:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n",
    "        chunk_with_context = torch.cat((self.context, chunk))\n",
    "        self.context = chunk[-self.context_length :]\n",
    "        return chunk_with_context\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Build inference pipeline from RNNTBundle.\n",
    "\n",
    "    Args:\n",
    "        bundle (torchaudio.pipelines.RNNTBundle): Bundle object\n",
    "        beam_width (int): Beam size of beam search decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):\n",
    "        self.bundle = bundle\n",
    "        self.feature_extractor = bundle.get_streaming_feature_extractor()\n",
    "        self.decoder = bundle.get_decoder()\n",
    "        self.token_processor = bundle.get_token_processor()\n",
    "\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        self.state = None\n",
    "        self.hypotheses = None\n",
    "\n",
    "    def infer(self, segment: torch.Tensor) -> str:\n",
    "        \"\"\"Perform streaming inference\"\"\"\n",
    "        features, length = self.feature_extractor(segment)\n",
    "        self.hypotheses, self.state = self.decoder.infer(\n",
    "            features, length, self.beam_width, state=self.state, hypothesis=self.hypotheses\n",
    "        )\n",
    "        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "345e7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Parameters\n",
    "    device = \"alsa\"\n",
    "    src = \"hw:4\"\n",
    "    n_channels = 6\n",
    "    \n",
    "    # Model info\n",
    "    bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\n",
    "    sample_rate = bundle.sample_rate\n",
    "    segment_length = bundle.segment_length * bundle.hop_length\n",
    "    context_length = bundle.right_context_length * bundle.hop_length\n",
    "    pipeline = Pipeline(bundle)\n",
    "    \n",
    "    \n",
    "    # Cache stream\n",
    "    cacher = ContextCacher(segment_length, context_length)\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    \n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def infer():\n",
    "        while True:\n",
    "            chunk = q.get()      \n",
    "            segment = cacher(chunk[:, 0])\n",
    "            transcript = pipeline.infer(segment)\n",
    "            print(transcript, end=\"\\r\", flush=True)\n",
    "    \n",
    "    q = ctx.Queue()\n",
    "    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n",
    "    p.start()\n",
    "    infer()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df1755-f2d4-46f4-98c6-cdcd33d404d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jd/anaconda3/envs/marmot/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/jd/anaconda3/envs/marmot/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'stream' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b1a53c3-6353-455f-b88b-a53984377a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "hop length: 160 \n",
      "segment length: 16 \n",
      "context length: 4 \n",
      "2560\n",
      "640\n",
      "<torchaudio.io._stream_reader.StreamReader object at 0x77951405ba60>\n"
     ]
    }
   ],
   "source": [
    "print(sample_rate)\n",
    "print(\"hop length: %s \" % bundle.hop_length)\n",
    "print(\"segment length: %s \" % bundle.segment_length)\n",
    "print(\"context length: %s \" %bundle.right_context_length)\n",
    "print(segment_length)\n",
    "print(context_length)\n",
    "\n",
    "print(streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e2b84-501e-44e4-a862-3bdadf7dfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stream\n",
    "\n",
    "\n",
    "# Visualize stream?\n",
    "\n",
    "\n",
    "# form beam at location (az/el/dist)\n",
    "\n",
    "\n",
    "# visualize separated beam\n",
    "\n",
    "\n",
    "# extract noise/common spectrum from all channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
