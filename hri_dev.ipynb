{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af1df5f-a8c8-4a06-8167-d182aa6a74ea",
   "metadata": {},
   "source": [
    "# CLIP recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ffc1f061-a5cf-4ad0-8f7c-1675688b24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c6e06083-142d-484d-83f7-690a8991814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f2171248-67b0-436d-a66e-2af47c7d7d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "914c3e6e-1b6b-41ef-9bba-4fdca1182ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:19<00:00, 18.4MiB/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "90f9a3d7-87e3-45ec-b8b7-b9c142218917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,   320,   900,  2533, 49407,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320, 17011,  2533, 49407,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   550,  9054,  2533, 49407,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(Image.open(\"../../test_img_from_cv_crop.jpg\")).unsqueeze(0).to(device)\n",
    "scene_text = clip.tokenize([\"an indoor scene\", \"an outdoor scene\", \"a picture of transportation\"]).to(device)\n",
    "\n",
    "clothing_text = clip.tokenize([\"a person wearing casual clothing\", \"a person wearing formal clothing\"]).to(device)\n",
    "\n",
    "emote_text = clip.tokenize([\"a happy person\", \"a neutral person\", \"an angry person\"]).to(device)\n",
    "print(emote_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "aa06164f-2b13-490c-a598-0c4ddf1f7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.919   0.02774 0.05347]]\n",
      "0.055628061294555664\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    scene_text_features = model.encode_text(scene_text)\n",
    "    clothing_text_features = model.encode_text(clothing_text)\n",
    "    emote_text_features = model.encode_text(emote_text)\n",
    "    \n",
    "    image_features = model.encode_image(image)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, emote_text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "stop = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Label probs:\", probs)\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6746a0-373a-4f11-95b4-5a526030d2fd",
   "metadata": {},
   "source": [
    "# GTSAM fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71c5a16d-1b6f-4f1f-b13a-2f2e63762a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam\n",
    "\n",
    "roles = [\"unknown\",\"pedestrian\",\"teammate\"]\n",
    "obs = [\"no_vest\",\"vest\"]\n",
    "id = 113\n",
    "n_obs = 0\n",
    "role_symbol = gtsam.Symbol('r',id)\n",
    "obs_symbol = gtsam.Symbol('o',id)\n",
    "\n",
    "# Prior\n",
    "role = gtsam.DiscreteDistribution((role_symbol.key(),len(roles)),\".75/.15/.1\")\n",
    "\n",
    "# Measurement likelihood\n",
    "p_det_role = gtsam.DiscreteConditional((obs_symbol.key(),len(obs)), [(role_symbol.key(),len(roles))], \"6/4 9/1 1/9\")\n",
    "\n",
    "# Measurement distribution\n",
    "p_obs = gtsam.DecisionTreeFactor((obs_symbol.key(),len(obs)),\".45 55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f8fdf33c-909e-472d-ab47-d6376940c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Prior\n",
      " P( r113 ):\n",
      " Choice(r113) \n",
      " 0 Leaf 0.75\n",
      " 1 Leaf 0.15\n",
      " 2 Leaf  0.1\n",
      "\n",
      "\n",
      "Discrete Conditional\n",
      " P( o113 | r113 ):\n",
      " Choice(r113) \n",
      " 0 Choice(o113) \n",
      " 0 0 Leaf  0.6\n",
      " 0 1 Leaf  0.4\n",
      " 1 Choice(o113) \n",
      " 1 0 Leaf  0.9\n",
      " 1 1 Leaf  0.1\n",
      " 2 Choice(o113) \n",
      " 2 0 Leaf  0.1\n",
      " 2 1 Leaf  0.9\n",
      "\n",
      "\n",
      "DecisionTreeFactor\n",
      " f[ (o113,2), ]\n",
      " Choice(o113) \n",
      " 0 Leaf 0.45\n",
      " 1 Leaf   55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(role)\n",
    "\n",
    "print(p_det_role)\n",
    "\n",
    "print(p_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3624f1fd-c90f-489a-8d0d-d67fce68bc19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DiscreteConditional::operator* called with overlapping frontal keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mp_det_role\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrole\u001b[49m\n\u001b[1;32m      2\u001b[0m role \u001b[38;5;241m=\u001b[39m gtsam\u001b[38;5;241m.\u001b[39mDiscreteDistribution(pred)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(role)\n",
      "\u001b[0;31mValueError\u001b[0m: DiscreteConditional::operator* called with overlapping frontal keys."
     ]
    }
   ],
   "source": [
    "pred = p_det_role*role\n",
    "role = gtsam.DiscreteDistribution(pred)\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b69b6-acd1-4237-ba35-77dc3eaa3ad3",
   "metadata": {},
   "source": [
    "# Audio dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b526bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchaudio.io import StreamReader\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edbc920e-b552-4f1a-bdde-d25cf365bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(q, format, src, segment_length, sample_rate):\n",
    "    \n",
    "    print(\"Building StreamReader...\")\n",
    "    streamer = torchaudio.io.StreamReader(src=src, format=format, option=option)\n",
    "    streamer.add_basic_audio_stream(frames_per_chunk=segment_length, sample_rate=sample_rate, format=\"s16p\", num_channels=6)\n",
    "\n",
    "    print(streamer.get_src_stream_info(0))\n",
    "    print(\"Streaming...\")\n",
    "    print()\n",
    "    for (chunk_a) in streamer.stream(timeout=-1, backoff=1.0):\n",
    "        q.put([chunk_a])\n",
    "\n",
    "\n",
    "class ContextCacher:\n",
    "    \"\"\"Cache the end of input data and prepend the next input data with it.\n",
    "\n",
    "    Args:\n",
    "        segment_length (int): The size of main segment.\n",
    "            If the incoming segment is shorter, then the segment is padded.\n",
    "        context_length (int): The size of the context, cached and appended.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segment_length: int, context_length: int):\n",
    "        self.segment_length = segment_length\n",
    "        self.context_length = context_length\n",
    "        self.context = torch.zeros([context_length])\n",
    "\n",
    "    def __call__(self, chunk: torch.Tensor):\n",
    "        if chunk.size(0) < self.segment_length:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n",
    "        chunk_with_context = torch.cat((self.context, chunk))\n",
    "        self.context = chunk[-self.context_length :]\n",
    "        return chunk_with_context\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Build inference pipeline from RNNTBundle.\n",
    "\n",
    "    Args:\n",
    "        bundle (torchaudio.pipelines.RNNTBundle): Bundle object\n",
    "        beam_width (int): Beam size of beam search decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bundle: torchaudio.pipelines.RNNTBundle, beam_width: int = 10):\n",
    "        self.bundle = bundle\n",
    "        self.feature_extractor = bundle.get_streaming_feature_extractor()\n",
    "        self.decoder = bundle.get_decoder()\n",
    "        self.token_processor = bundle.get_token_processor()\n",
    "\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        self.state = None\n",
    "        self.hypotheses = None\n",
    "\n",
    "    def infer(self, segment: torch.Tensor) -> str:\n",
    "        \"\"\"Perform streaming inference\"\"\"\n",
    "        features, length = self.feature_extractor(segment)\n",
    "        self.hypotheses, self.state = self.decoder.infer(\n",
    "            features, length, self.beam_width, state=self.state, hypothesis=self.hypotheses\n",
    "        )\n",
    "        transcript = self.token_processor(self.hypotheses[0][0], lstrip=False)\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "345e7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Parameters\n",
    "    device = \"alsa\"\n",
    "    src = \"hw:4\"\n",
    "    n_channels = 6\n",
    "    \n",
    "    # Model info\n",
    "    bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\n",
    "    sample_rate = bundle.sample_rate\n",
    "    segment_length = bundle.segment_length * bundle.hop_length\n",
    "    context_length = bundle.right_context_length * bundle.hop_length\n",
    "    pipeline = Pipeline(bundle)\n",
    "    \n",
    "    \n",
    "    # Cache stream\n",
    "    cacher = ContextCacher(segment_length, context_length)\n",
    "    \n",
    "    \n",
    "    # Inference\n",
    "    \n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def infer():\n",
    "        while True:\n",
    "            chunk = q.get()      \n",
    "            segment = cacher(chunk[:, 0])\n",
    "            transcript = pipeline.infer(segment)\n",
    "            print(transcript, end=\"\\r\", flush=True)\n",
    "    \n",
    "    q = ctx.Queue()\n",
    "    p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n",
    "    p.start()\n",
    "    infer()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df1755-f2d4-46f4-98c6-cdcd33d404d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/jd/anaconda3/envs/marmot/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/jd/anaconda3/envs/marmot/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'stream' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b1a53c3-6353-455f-b88b-a53984377a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "hop length: 160 \n",
      "segment length: 16 \n",
      "context length: 4 \n",
      "2560\n",
      "640\n",
      "<torchaudio.io._stream_reader.StreamReader object at 0x77951405ba60>\n"
     ]
    }
   ],
   "source": [
    "print(sample_rate)\n",
    "print(\"hop length: %s \" % bundle.hop_length)\n",
    "print(\"segment length: %s \" % bundle.segment_length)\n",
    "print(\"context length: %s \" %bundle.right_context_length)\n",
    "print(segment_length)\n",
    "print(context_length)\n",
    "\n",
    "print(streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e2b84-501e-44e4-a862-3bdadf7dfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stream\n",
    "\n",
    "\n",
    "# Visualize stream?\n",
    "\n",
    "\n",
    "# form beam at location (az/el/dist)\n",
    "\n",
    "\n",
    "# visualize separated beam\n",
    "\n",
    "\n",
    "# extract noise/common spectrum from all channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
