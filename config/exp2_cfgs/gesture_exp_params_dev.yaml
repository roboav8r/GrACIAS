/**:
  ros__parameters:

    config_name: artag_role_gesture_cmd

    use_sim_time: true
    tracker_frame: 'philbart/base_link'
    mic_frame: 'philbart/base_link'
    microphone_frame_id: 'philbart/base_link'
    artag_frame: 'oak_rgb_camera_optical_frame'
  
    # Signal acquisition
    n_total_channels: 16
    channel_indices_used: [0, 1, 2, 3, 4, 5, 6, 7]
    sample_rate: 44100
    hop_size: 44100
    frame_size: 44100 # publish 1 second every second

    # Sound source localization
    speed_sound: 343.
    n_sources: 2
    n_fft: 512
    f_min: 300
    f_max: 8000
    doa_dimension: 2
    array_x_pos: [0.43, 0.43, -0.34, -0.34, 0., 0., 0., 0.]
    array_y_pos: [-0.28, 0.28, 0.30, -0.30, -0.29, 0.29, -0.11, 0.11]
    array_z_pos: [0.395, 0.395, 0.395, 0.395, 0.610, 0.610, 0.660, 0.660]
    ssl_algo: 'MUSIC'

    # Speech activity detection & recognition
    n_silent_frames: 2
    trigger_time: 0.25
    search_time: 0.25
    allowed_gap: 0.25
    pre_trigger_time: 0.25
    min_voice_samples: 31200
    src_match_thresh_rad: .175 # .5 = ~28.6 degrees # .175 rad = ~10 degrees

    # Best overall audio params from 24 Oct mini speech eval set
    lexicon_package: 'ros_audition'
    lexicon_file: 'config/rocog_lexicon_full_phrases.txt'
    trigger_level: 5.
    am_bundle: 'WAV2VEC2_ASR_LARGE_960H'
    lm_weight: 1.
    word_score: 1.
    sil_score: 0.


### UNIMODAL PROCESSING - VISION
oak:
  ros__parameters:
    camera:
      i_enable_imu: true
      i_enable_ir: true
      i_nn_type: spatial
      i_pipeline_type: RGBD
    stereo:
      i_subpixel: true
    nn:
      i_nn_config_path: depthai_ros_driver/yolo
      i_disable_resize: true
    rgb:
      i_preview_size: 416

depthai_img_preproc_node:
  ros__parameters:
    nn_img_size: 416
    x_correction_factor: 1.
    y_correction_factor: 1.
    detector_frame: 'oak_rgb_camera_optical_frame'
    labels: ["person", "bicycle","car","motorbike","aeroplane","bus","train","truck","boat","traffic light","fire hydrant","stop sign","parking meter","bench","bird","cat","dog","horse","sheep","cow","elephant","bear","zebra","giraffe","backpack","umbrella","handbag","tie","suitcase","frisbee","skis","snowboard","sports ball","kite","baseball bat","baseball glove","skateboard","surfboard","tennis racket","bottle","wine glass","cup","fork","knife","spoon","bowl","banana","apple","sandwich","orange","broccoli","carrot","hot dog","pizza","donut","cake","chair","sofa","pottedplant","bed","diningtable","toilet","tvmonitor","laptop","mouse","remote","keyboard","cell phone","microwave","oven","toaster","sink","refrigerator","book","clock","vase","scissors","teddy bear","hair drier","toothbrush"]

clip_scene_rec:
  ros__parameters:
    num_est_interval_samples: 15
    scene_labels: ['campus','courtyard','lab','lobby']
    scene_descriptions: ['a picture of a public outdoor area with high social activity',
                         'a picture of a private outdoor area with low social activity',
                         'a picture of a private indoor area with low social activity',
                         'a picture of a public indoor area with high social activity']
    clip_model: 'ViT-L/14'

clip_vis_rec_server:
  ros__parameters:
    clip_model: 'ViT-L/14'
    object_classes: ['person']

    person:
      attributes:
        variables: ['']

      states:
        variables: ['role']
        role:
          labels: ['pedestrian','teammate','supervisor']
          descriptions: ['a picture of a person', 
                         'a picture of a person wearing a university of texas robotics shirt and orange texas hat', 
                         'a picture of a person wearing a university of texas robotics shirt and orange texas hat and a yellow safety vest']
      
      comms:
        labels: ['']
        gesture_descriptions: ['']
        probs: []
        # p(command_detected|command_issued)
        gesture_sensor_model_coeffs: []        
        verbal_sensor_model_coeffs: []

# gesture recognition
gesture_keypoint_node:
  ros__parameters:
    model_name: 'yolo11n-pose' # YOLO11n-pose, YOLO11s-pose, YOLO11m-pose, YOLO11l-pose, YOLO11x-pose
    keypoints_per_frame: 17
    keypoint_dimension: 3
    # keypoint_buffer_length: 40
    # target_frames_per_second: 16.0
    # buffer_publish_rate: 10.
    # visualize_keypoints: True


### UNIMODAL PROCESSING - AUDITION
audio_acq_node:
  ros__parameters:
    n_channels: 16
    src: 'plughw:1,0'
    sample_rate: 44100
    hop_size: 44100
    frame_size: 44100 # publish 1 second every second
    microphone_frame_id: 'philbart/base_link'

audio_scene_rec:
  ros__parameters:
    n_channels: 16
    sample_rate: 44100
    downsample_rate: 32000
    frame_size: 44100
    scene_size: 44100
    scene_index: [0]
    labels: ['airport', 'bus', 'metro', 'metro_station', 'park', 'public_square', 'shopping_mall', 'street_pedestrian', 'street_traffic', 'tram']
    scene_est_interval: 1.0
    wandb_id: 'fjycgepm'

    # Model path params
    project_name: DCASE23_Task1
    wandb_id: 'c0a7nzin'

    # Mel pre-processing parameters
    resample_rate: 32000
    window_size: 3072
    hop_size: 500
    n_fft: 4096
    n_mels: 256
    freqm: 48
    timem: 0
    fmin: 0
    fmax: 0
    fmin_aug_range: 1
    fmax_aug_range: 1000

    # Model parameters
    n_classes: 10
    in_channels: 1
    base_channels: 32
    channels_multiplier: 2.3
    expansion_rate: 3


### MULTIMODAL PROCESSING - SCENE RECOGNITION
/bayes_fused_scene_est:
  ros__parameters:

    scene_labels: ['campus','courtyard','lab','lobby']
    scene_prior: [.25,.25,.25,.25]

    sensor_names: ['clip','dcase23']

    clip:
      obs_labels: ['campus','courtyard','lab','lobby']
      sensor_model_coeffs: [35272., 3382., 1., 1.,
                            154., 42755., 1., 1.,
                            1., 54., 41930., 211.,
                            1., 15., 4906., 36521.]
      topic: 'clip_scene_category'

    dcase23:
      obs_labels: ['airport', 'bus', 'metro', 'metro_station','park','public_square','shopping_mall','street_pedestrian','street_traffic','tram']
      sensor_model_coeffs: [13., 19., 49., 14., 206., 7., 1., 1., 37., 1941.,
                            6., 274., 9., 11., 767., 3., 2., 2., 19., 1382.,
                            16., 157., 2111., 43., 80., 2., 1., 1., 65., 31.,
                            479., 57., 191., 468., 974., 1., 68., 18., 21., 175.]
      topic: 'audio_scene_category'


### MULTIMODAL PROCESSING - SEMANTIC FUSION NODE
semantic_fusion_node:
  ros__parameters:
    tracker_frame: 'philbart/base_link'
    mic_frame: 'philbart/base_link'
    artag_frame: 'oak_rgb_camera_optical_frame'
    gesture_kp_frame: 'oak_rgb_camera_optical_frame'

    x_label_offset: .0 # meters
    y_label_offset: -.75 # meters
    z_label_offset: .75 # meters

    update_loop_time_sec: 0.1
    pub_loop_time_sec: .25
    
    role_rec_methods: ['visual']
    command_rec_methods: ['gesture']

    sensor_names: ['clip_role_rec','lstm_gesture_rec']
    sensor_symbol: 'z' # Letter to uniquely identify the variable in GTSAM.
    sensors:

      clip_role_rec:
        type: 'vision'
        topic: ''

        update_method: 'time'
        update_threshold: .9 # 

        # GTSAM fusion variables
        role_obs_labels: ['pedestrian','teammate','supervisor']
        role_obs_model_coeffs: [.721171, .278790, .000040,
                                .033467, .966514, .000019,
                                .001246, .011736, .987018]

      lstm_gesture_rec:
        type: 'gesture'
        topic: '/pose_keypoints'

        update_method: 'time'
        update_threshold: .25 # 

        match_threshold: .5 # radians. ~30 degrees

        # GTSAM fusion variables
        comm_obs_labels: ['advance', 'attention', 'follow-me', 'halt', 'move-forward', 'move-in-reverse', 'rally']
        comm_obs_model_coeffs: [0.142857143, 0.142857143, 0.142857143, 0.142857143, 0.142857143, 0.142857143, 0.142857143, # p(obs|none)
                                0.921052632, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895,
                                0.013157895, 0.921052632, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895,
                                0.013157895, 0.013157895, 0.921052632, 0.013157895, 0.013157895, 0.013157895, 0.013157895,
                                0.013157895, 0.013157895, 0.013157895, 0.921052632, 0.013157895, 0.013157895, 0.013157895,
                                0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.921052632, 0.013157895, 0.013157895,
                                0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.921052632, 0.013157895,
                                0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.013157895, 0.921052632]

    objects_of_interest: ['person']
    person:
      attributes:
        variables: ['']

      states:
        variables: ['role']
        role:
          symbol: 'r' # Letter to uniquely identify the variable in GTSAM.
          labels: ['pedestrian','teammate','supervisor']
          probs: [1., 1., 1.]
          upper_prob_limit: .999
          lower_prob_limit: .001
      
      comms: 
        symbol: 'c' # Letter to uniquely identify the variable in GTSAM.
        labels: ['none', 'advance', 'attention', 'follow-me', 'halt', 'move-forward', 'move-in-reverse', 'rally']
        probs: [1., .01, .01, .01, .01, .01, .01, .01]  
        upper_prob_limit: .999
        lower_prob_limit: .001

      # Specific to person
      window_length: 24
      model_input_dim: 51

### EXPERIMENT MANAGER NODE
/**:
  ros__parameters:
    exp_configs: ['config/exp2_cfgs/exp2_ar_role_gesture_cmd.yaml']
    mcap_dir: 'sit_int_ws/src/situated_interaction/bags/e1_gesture_dev'
    result_dir: 'sit_int_ws/src/situated_interaction/results/exp2_hierarchical_cmd/gesture_rec'